# Jamba Model in Easy Pytorch

This repo is an _unofficial_ implementation of the `Jamba` model as introduced in [Lieber et al., (2024)](https://arxiv.org/abs/2403.19887). One can find the official project webpage [here](https://www.ai21.com/jamba). This repo is developed mainly for didactic purposes to spell out the details of the how to hybridize `SSM` with `Transformers`.

# Usage

# Roadmap

- [ ] Put all the essential pieces together: `Mamba`, `MoE`.
- [ ] Add functioning training script (Lightning)
- [ ] Show some results

# Citations

```bibtex
@article{lieber2024jamba,
  title={Jamba: A Hybrid Transformer-Mamba Language Model},
  author={Lieber, Opher and Lenz, Barak and Bata, Hofit and Cohen, Gal and Osin, Jhonathan and Dalmedigos, Itay and Safahi, Erez and Meirom, Shaked and Belinkov, Yonatan and Shalev-Shwartz, Shai and others},
  journal={arXiv preprint arXiv:2403.19887},
  year={2024}
}
```
